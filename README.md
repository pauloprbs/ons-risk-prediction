# üîã Previs√£o de Risco de D√©ficit Energ√©tico em Goi√°s
**Machine Learning para Pol√≠ticas P√∫blicas**

---

## üìå Vis√£o Geral

Este projeto tem como objetivo desenvolver um modelo de machine learning para classificar o risco di√°rio de d√©ficit de energia el√©trica em Goi√°s. O modelo deve categorizar cada dia em tr√™s n√≠veis de risco (**baixo**, **m√©dio** ou **alto**), servindo como uma ferramenta de apoio √† decis√£o para a seguran√ßa do suprimento energ√©tico.

Este reposit√≥rio cont√©m duas abordagens para este desafio:

1. Um processo de **an√°lise explorat√≥ria e modelagem local** usando notebooks Jupyter.

2.  Um **pipeline de dados de produ√ß√£o em nuvem (ELT)** usando Snowflake, dbt, Airflow e Airbyte, projetado para automatizar a engenharia de features e entregar a tabela de modelagem final para o treinamento de modelos na AWS.
---

## üöÄ Abordagem 1: An√°lise Explorat√≥ria e Modelagem Local (Notebooks)

Esta abordagem descreve o fluxo de trabalho de pesquisa e desenvolvimento, executado localmente para explorar os dados e validar a viabilidade do modelo.

1. **Configura√ß√£o do Ambiente**

Clone o reposit√≥rio e configure o ambiente virtual:

```bash
# Clone o reposit√≥rio
git clone [URL_DO_REPOSITORIO]
cd ons-risk-prediction

# Crie e ative o ambiente virtual
python -m venv venv
source venv/bin/activate   # Mac/Linux
.\venv\Scripts\activate    # Windows

# Instale as depend√™ncias
pip install -r requirements.txt
```

2. **Defini√ß√£o do Per√≠odo de An√°lise**

Edite o arquivo ```config.py``` na raiz do projeto para definir o intervalo de tempo desejado para a an√°lise. O arquivo de configura√ß√£o agora permite uma execu√ß√£o din√¢mica.

- ```START_YEAR``` e ```START_MONTH``` definem o in√≠cio do per√≠odo de coleta (ex: 2010, 1).

- ```USE_CURRENT_DATE = True``` far√° com que o pipeline colete dados at√© o m√™s e ano atuais.

- ```USE_CURRENT_DATE = False``` usar√° os valores manuais END_YEAR e END_MONTH para definir um per√≠odo fixo.

```python
# Exemplo em config.py
from datetime import datetime

# --- Per√≠odo de An√°lise ---
START_YEAR = 2010
START_MONTH = 1

# --- Controle Autom√°tico ---
USE_CURRENT_DATE = True

# Datas finais
if USE_CURRENT_DATE:
    today = datetime.today()
    END_YEAR = today.year
    END_MONTH = today.month
else:
    END_YEAR = 2025
    END_MONTH = 10
```

3. **Coleta Automatizada de Dados**

Execute os scripts na pasta ```/scripts``` para baixar todos os dados brutos necess√°rios para a pasta ```data/raw/```. Eles ler√£o a configura√ß√£o do ```config.py```.

```bash
# Baixa os arquivos CSV do S3 da ONS (Gera√ß√£o, Rede, H√≠dricos, etc.)
python scripts/download_data.py

# Extrai os dados de Carga (Programada e Verificada) da API da ONS
python scripts/extract_carga_api.py

# Baixa os dados meteorol√≥gicos da API NASA POWER
python scripts/download_weather_data.py
```

4. **Engenharia de Features e Modelagem**

Execute os notebooks Jupyter em sequ√™ncia, do 01 ao final. Cada notebook realiza uma etapa do processamento e salva seu resultado, que √© usado pelo notebook seguinte:

```01-EDA-Variavel-Alvo-Interrupcao.ipynb```
```02-EDA-Carga-Energia.ipynb```
```03-EDA-Geracao.ipynb```
```04-EDA-Rede.ipynb```
```05-EDA-Hidrica.ipynb```
```06-Feature-Engineering-Avancada.ipynb```
```07-Features-Adicionais.ipynb```
```08-Features-Meteorologicas.ipynb```
```09-Modelagem-XGBoost.ipynb```
```10-Modelagem-LTSM.ipynb```
```11-Modelagem-Unbalanced-Learning.ipynb```
```12-Ensemble.ipynb```

---

## ‚òÅÔ∏è Abordagem 2: Pipeline de Produ√ß√£o em Nuvem (Snowflake + dbt + Airflow)

Esta abordagem transforma a l√≥gica explorat√≥ria dos notebooks em um pipeline de dados ELT (Extract, Load, Transform) robusto, automatizado e escal√°vel, pronto para um ambiente de produ√ß√£o.

- **Ferramentas Utilizadas**:

- **Snowflake**: Data Warehouse em nuvem, onde os dados ser√£o armazenados e transformados.

- **dbt (Data Build Tool)**: Ferramenta para gerenciar as transforma√ß√µes SQL (a l√≥gica dos notebooks).

- **Airflow**: Orquestrador para agendar e executar o pipeline automaticamente.

- **Airbyte**: Ferramenta de ingest√£o de dados (para os dados din√¢micos, em um pr√≥ximo passo).

**Ingest√£o da Camada RAW**

Os passos a seguir descrevem como configurar o Data Warehouse no Snowflake e executar a **carga inicial (snapshot)** de todos os dados brutos. Esta √© a funda√ß√£o necess√°ria antes de construir as transforma√ß√µes com dbt.

**Passo 1: Gerar Arquivos de Snapshot (Local)**

Antes de popular o Snowflake, precisamos ter todos os arquivos brutos dispon√≠veis localmente.

1. **Definir Per√≠odo no** ```config.py```: Certifique-se de que o config.py est√° configurado para o per√≠odo desejado (ex: ```USE_CURRENT_DATE = True``` para pegar tudo at√© hoje).

2. **Baixar Dados Hist√≥ricos**: Execute python scripts/download_data.py.

3. **Gerar Dados das APIs**: Execute python ```scripts/extract_carga_api.py``` e ```python scripts/download_weather_data.py```.

Ao final, sua pasta ```data/raw/``` deve conter todos os arquivos CSV e Parquet necess√°rios.

**Passo 2: Configurar o Ambiente no Snowflake**

No Snowflake, execute o script ```snowflake_scripts_ons/01_setup.sql```.

- O que faz:

    - ```CREATE DATABASE```: Cria os bancos ```RAW_DB```, ```STAGING_DB``` e ```CORE_DB```.
    - ```CREATE WAREHOUSE```: Cria o warehouse ```DBT_WH```.
    - ```CREATE FILE FORMAT```: Cria os formatos ```ONS_CSV_FORMAT``` e ```ONS_PARQUET_FORMAT```.
    - ```CREATE STAGE```: Cria o stage ```ONS_RAW_STAGE``` de forma permanente, garantindo que os arquivos de upload n√£o sejam apagados.

**Passo 3: Criar as Tabelas da Camada RAW**

Execute o script ```snowflake_scripts_ons/02_create_raw_tables.sql```.

- O que faz:

    - ```CREATE OR REPLACE TABLE```: Cria todas as 12 tabelas (ex: ```GERACAO_USINA_RAW```, ```INTERRUPCAO_CARGA_RAW```, ```CLIMA_GO_DIARIO_RAW```, etc.) no schema ```RAW_DB.ONS_DATA```.
    - **Importante**: As tabelas s√£o criadas com todas as colunas como ```VARCHAR``` (ou tipos de dados compat√≠veis) para garantir que a carga de dados brutos nunca falhe por tipos de dados inesperados.

**Passo 4: Fazer Upload dos Arquivos para o Stage**

Esta etapa √© manual e usa a interface Web do Snowflake (Snowsight).

Navegue at√© o stage ```RAW_DB.ONS_DATA.ONS_RAW_STAGE```.

Clique em "+ Files" e fa√ßa o upload de todos os arquivos da sua pasta ```data/raw/``` para os seus respectivos diret√≥rios no stage, conforme o mapeamento abaixo:

| Fonte de Dados (Notebook) | Diret√≥rio no Stage | Arquivos a Serem Carregados (da pasta `data/raw/`) |
| :--- | :--- | :--- |
| `02-EDA-Carga-Energia.ipynb` | `carga_verificada/` | `carga_verificada_go.parquet` |
| `02-EDA-Carga-Energia.ipynb` | `carga_programada/` | `carga_programada_go.parquet` |
| `08-Features-Meteorologicas.ipynb` | `clima/` | `clima_go_diario.csv` |
| `01-EDA-Variavel-Alvo-Interrupcao.ipynb` | `interrupcao/` | `INTERRUPCAO_CARGA.csv` |
| `03-EDA-Geracao.ipynb` | `geracao_usina/` | `GERACAO_USINA_*.csv` |
| `04-EDA-Rede.ipynb` | `restricao_eolica/` | `RESTRICAO_COFF_EOLICA*.csv` |
| `04-EDA-Rede.ipynb` | `restricao_fotovoltaica/` | `RESTRICAO_COFF_FOTOVOLTAICA*.csv` |
| `04-EDA-Rede.ipynb` | `intercambio_nacional/` | `INTERCAMBIO_NACIONAL_*.csv` |
| `05-EDA-Hidrica.ipynb` | `ear_diario/` | `EAR_DIARIO_SUBSISTEMA_*.csv` |
| `05-EDA-Hidrica.ipynb` | `ena_diario/` | `ENA_DIARIO_SUBSISTEMA_*.csv` |
| `07-Features-Adicionais.ipynb` | `cmo_semanal/` | `CMO_SEMANAL_*.csv` |
| `07-Features-Adicionais.ipynb` | `disponibilidade_usina/` | `DISPONIBILIDADE_USINA_*.csv` |

**Passo 5: Ingest√£o de Dados (Load)**

Execute o script ```snowflake_scripts_ons/03_load_raw_data.sql```.

- O que faz:

    - Executa 12 comandos ```COPY INTO``` ... que carregam os arquivos das subpastas do stage para as tabelas RAW correspondentes.
    - Usa ```MATCH_BY_COLUMN_NAME = 'CASE_INSENSITIVE'``` para os arquivos Parquet, garantindo o mapeamento correto das colunas.
    - Usa ```PURGE = FALSE``` para que os arquivos no stage n√£o sejam apagados ap√≥s a carga, permitindo re-execu√ß√µes para desenvolvimento.
    - Finaliza com uma consulta ```UNION ALL``` que mostra a contagem de linhas em todas as 12 tabelas, validando que a ingest√£o foi bem-sucedida.

---

## üìä Estrutura e Descobertas do Projeto

- **Coleta e Configura√ß√£o (Scripts e config.py)**: O projeto demonstrou a import√¢ncia de uma pipeline de dados robusta. A coleta foi automatizada e centralizada para lidar com dezenas de arquivos de m√∫ltiplos anos (desde 2010), superando inconsist√™ncias nos dados de origem.
- **Defini√ß√£o do Alvo (Notebook 01):** A vari√°vel alvo do projeto, `nivel_risco`, foi derivada da m√©trica `val_energianaosuprida_mwh` (Energia N√£o Suprida), escolhida por ser o indicador mais direto de um d√©ficit no sistema. Para converter esta m√©trica cont√≠nua (MWh de d√©ficit di√°rio) em tr√™s classes de risco discretas (**baixo**, **m√©dio**, **alto**), os limiares de classifica√ß√£o foram definidos estatisticamente. Foram utilizados os quantis (como P34 e P67, conforme explorado no notebook) calculados sobre os dias que registraram algum d√©ficit, de forma a contornar o desbalanceamento natural dos dados e viabilizar a modelagem de 3 classes.
- **Engenharia de Features (Notebooks 02 a 08)**: Foi constru√≠da uma tabela de dados abrangente, cobrindo os pilares de Demanda (Carga), Oferta (Gera√ß√£o), Rede (Restri√ß√£o, Interc√¢mbio), Hidrologia (EAR/ENA), Economia (CMO) e Meteorologia (Clima). Notavelmente, foram adicionadas features avan√ßadas de janela deslizante e intera√ß√£o (Notebook 06).
- **Modelagem (Notebooks 09 a 12)**: Foram implementadas t√©cnicas avan√ßadas para lidar com o desbalanceamento de classes (Notebook 11). Foram treinados e otimizados modelos, incluindo XGBoost (Notebook 09), LTSM (Notebook 10) e t√©cnicas de Ensemble (Notebook 12).

---

## üìâ Conclus√µes Finais

*1. Principal Descoberta: O Problema da "Agulha no Palheiro."* A principal conclus√£o do projeto √© que, mesmo com um dataset abrangendo **mais de uma d√©cada (desde 2010)** e uma engenharia de features complexa, a previs√£o de 3 classes de risco √© extremamente desafiadora. Os eventos de risco "m√©dio" e "alto" s√£o t√£o raros que os modelos de machine learning, embora tecnicamente funcionais, apresentaram um baixo poder preditivo (Recall nulo ou pr√≥ximo de zero) para essas classes.

*2. An√°lise de Insights: Identificando os Fatores-Chave de Risco*: Uma das sa√≠das mais valiosas do projeto, obtida atrav√©s da modelagem (incluindo XGBoost e Ensemble), √© o ranking de Import√¢ncia das Features. Esta an√°lise revelou quais indicadores s√£o os mais sens√≠veis ao estresse do sistema el√©trico. Consistentemente, vari√°veis ligadas √† seguran√ßa e ao custo do sistema, como:

* `ear_percentual_seco` (n√≠vel dos reservat√≥rios)
* `cmo_semanal_seco` (pre√ßo da energia)
* `saldo_intercambio_seco` (depend√™ncia de outras regi√µes)
* Features de tend√™ncia, como `carga_media_7d` (m√©dia m√≥vel da carga)

... apareceram como as mais importantes. Este resultado fornece um insight acion√°vel sobre quais m√©tricas s√£o cruciais e devem ser monitoradas com mais aten√ß√£o para uma gest√£o proativa do risco energ√©tico.