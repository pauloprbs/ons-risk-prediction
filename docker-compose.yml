version: '3.8'

services:
  # -------------------
  # Airflow Services
  # -------------------
  airflow-db:
    image: postgres:13
    container_name: airflow-db
    environment:
      - POSTGRES_USER=${AIRFLOW_DB_USER:-airflow}
      - POSTGRES_PASSWORD=${AIRFLOW_DB_PASSWORD:-airflow}
      - POSTGRES_DB=${AIRFLOW_DB_NAME:-airflow}
    ports:
      - "5434:5432"
    volumes:
      - airflow-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_DB_USER:-airflow}"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-init:
    image: apache/airflow:2.7.0
    container_name: airflow-init
    user: "0:0"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@airflow-db:5432/${AIRFLOW_DB_NAME:-airflow}
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=${AIRFLOW_USER:-admin}
      - _AIRFLOW_WWW_USER_PASSWORD=${AIRFLOW_PASSWORD:-admin}
    volumes:
      - ./airflow_ons/dags:/opt/airflow/dags
      - ./airflow_ons/logs:/opt/airflow/logs
      - ./airflow_ons/plugins:/opt/airflow/plugins
      - ./dbt_ons:/opt/airflow/dbt_ons
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /opt/airflow/logs /opt/airflow/dags /opt/airflow/plugins
        chown -R 50000:0 /opt/airflow
        # Instala as dependências do dbt dentro do container do Airflow
        pip install dbt-snowflake
        exec /entrypoint airflow version
    depends_on:
      airflow-db:
        condition: service_healthy

  airflow-webserver:
    image: apache/airflow:2.7.0
    container_name: airflow-webserver
    user: "50000:0"
    command: webserver
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@airflow-db:5432/${AIRFLOW_DB_NAME:-airflow}
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW__WEBSERVER__SECRET_KEY:-changeme}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./airflow_ons/dags:/opt/airflow/dags
      - ./airflow_ons/logs:/opt/airflow/logs
      - ./airflow_ons/plugins:/opt/airflow/plugins
      - ./dbt_ons:/opt/airflow/dbt_ons
    depends_on:
      - airflow-init
    restart: always
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    image: apache/airflow:2.7.0
    container_name: airflow-scheduler
    user: "50000:0"
    command: scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://${AIRFLOW_DB_USER:-airflow}:${AIRFLOW_DB_PASSWORD:-airflow}@airflow-db:5432/${AIRFLOW_DB_NAME:-airflow}
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
    volumes:
      - ./airflow_ons/dags:/opt/airflow/dags
      - ./airflow_ons/logs:/opt/airflow/logs
      - ./airflow_ons/plugins:/opt/airflow/plugins
      - ./dbt_ons:/opt/airflow/dbt_ons
    depends_on:
      - airflow-init
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$${HOSTNAME}\""]
      interval: 30s
      timeout: 10s
      retries: 5

  # ----------------------------------------------------
  # dbt (Snowflake) - Serviço para Desenvolvimento Local
  # ----------------------------------------------------
  # Este serviço é para você executar comandos dbt manualmente no seu terminal, se precisar.
  # A orquestração automática será feita pelo Airflow.
  dbt:
    image: ghcr.io/dbt-labs/dbt-snowflake:latest
    container_name: dbt_ons_service
    volumes:
      - ./dbt_ons:/usr/app
      - ~/.dbt/profiles.yml:/root/.dbt/profiles.yml
    working_dir: /usr/app
    # Removemos as variáveis de ambiente do Snowflake, pois o dbt usará o profiles.yml
    entrypoint: ["/bin/sh", "-c", "echo 'dbt container ready. Run commands like: docker-compose exec dbt_ons_service dbt run' && while true; do sleep 30; done"]

volumes:
  airflow-db-data: